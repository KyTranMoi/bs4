from googletrans import Translator

def translate_text(text):
    translator = Translator()
    translated = translator.translate(text, src='en', dest='vi')
    return translated.text

# Sử dụng hàm để dịch một đoạn văn bản
english_text = '    "content": "Introduction\n  Recently Deep Learning has gained mass popularity in the world of science computing When we fire up Alexa or Siri sometimes we wonder how the machine is able to selfactualize decisions and make right choices \n  Well Deep learning and AI enable the machines to perform these functions making our lives easier and simpler Deep learning often tends to alleviate the levels of customer experience and the aura of premiumness is unmatched\n\n  What is Deep learning\n  Deep learning is a subset function of AI that imitates the functioning of the human brain and borrows the skill of processing data for use in detecting objects recognizing speech translating languages and making decisions It is a type of machine learning that works based on the structure and functioning of the human brain\n  It uses artificial neural networksANNs to perform sophisticated and intricate computations on enormous amounts of data\n  Deep learning is a subset of artificial intelligence that has networks capable of unsupervised learning from data that is unstructured or unlabeled\n  Deep learning has evolved handinhand with the digital era which has brought about a revolution in terms of data extraction in all forms and from every region of the world\n  This data renowned  as Big data is drawn from the sensational sources like social media internet search engines ecommerce platforms and online cinemas among others\n  Also catch Deep Learning vs Machine Learning\n  What is a Neural network\n  A Neural network is a web of artificial neurons known as nodes which is structured like a human brain These nodes are stacked next to each other in three layers\n  The input layer\nThe hidden layers\nThe output layer\n    What are Top Deep Learning Algorithms\n  Convolutional Neural Network\n  Yann LeCun developed the first CNN in 1988 and named it LeNet Then it was primarily used for recognizing characters like ZIP codes and digits \n  CNNs now  known as ConvNets consist of multiple layers structure and are mostly used for image processing and object detection\nCNN has a convolution layer that has several filters to deal with its intricacy and perform the convolution operation \nCNNs also have a Rectified Linear Unit ReLU layer to perform operations on elements and present a rectified feature map as an output\nCNN structure Source\nThe rectified feature map is next fed into a pooling layer and as the name suggests this layer converts the resulting twodimensional arrays from the pooled feature map into a single continuous linear vector by flattening it \n  2 Long Short Term Memory Networks\n  LSTMs are a subset of Recurrent Neural Networks RNN that are specialised in learning and memorizing longterm information By default LSTMs are supposed  to recall past information for long periods of time\n  LSTMs have a chainlike structure where four unique layers are stacked \nLSTMs are typically used for timeseries predictions speech synthesis language modeling and translation music composition and pharmaceutical development\nLSTM structure Source\nThey are programmed to forget irrelevant parts of the data and selectively update the cellstate values\n  3 Recurrent Neural Networks\n  As the Wikipedia page defines a recurrent neural network RNN it is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence This allows it to exhibit temporal dynamic behavior\n  Due to this dynamic behaviour the output from LSTMs is allowed to be fed as input here\n  The output from the LSTM becomes an input to the current phase allowing to  memorize previous inputs due to its efficient internal memory \nRNNs are mostly used for image captioning timeseries analysis naturallanguage processing handwriting recognition and machine translation\nWorking of RNNssource\nRNNs can process inputs of varied lengths The more the computation the more will be the possibility of information to be gathered and in addition the model size does not increase with the input size\n  4 Generative Adversarial Networks\n  GANs are generative deep learning algorithms that are responsible for producing new data instances that identify with the training data provided \n  GANs have two main components a generator which is used to generate fake data and a discriminator which learns from that false information  \nGANs expertise are used to generate realistic images and cartoon characters create photographs of human faces and render 3D objects \nYou might have noticed GANs logos on video games as developers use GANs to upgrade lowresolution 2D textures in vintage video games by recreating them in surreal 4K or higher resolutions via image training \nThey are also used to improve astronomical images and simulate gravitational lensing for darkmatter research\nWorking of GANs source\nDuring the training period the Discriminator learns to distinguish between real and fake data and is able to rectify whenever the Generator produces fake data\n  5 Radial basis function networks\n  RBFNs are an example of artificial neural networks mainly used for function approximation problems \n  Radial basis function networks are considered better from other neural networks because of their universal approximation method and faster learning speed\nAn RBF network is a special type of feed forward neural network It consists of three different layers namely the input layer the hidden layer and the output layer\nAn RBF network with 10 nodes in its hidden layer is chosen The training of the RBF model is terminated once the calculated error boils down to ideal values ie 001 or number of training iterations ie 500 already was completed\n Working of RBFNsSource\nRBFNs tend to perform classification by measuring the inputs congruency to examples from the training set The function finds the total sum of the inputs and the output layer receives one node per category or class of data\nThe neurons in the hidden layer work on the principles of Gaussian transfer functions which produces outputs that are inversely proportional to their distance from the neurons centerThe networks output is an interwebbed combination of the inputs radialbasis functions and the neurons parameters\n  6 Multilayer Perceptrons\n  MLPs belong to the family of feedforward neural networks with multiple layers of perceptrons that have different functions \n  MLPs consist of an input layer and an output layer that are fully connected with the hidden layers in between \nThey have the same number of input and output layers but may have multiple hidden layers which act as the true computation engine of MLPs \nThey are used to build speechrecognition financial prediction and carry out data compression\nThe data is fed to the input layer of the network Then the layers of neurons form a pattern which enables the signal to pass in one direction\nMLPs compute the  input with the entities that exist between the input layer and the hidden layers\nWorking of MLPssource\nActivation functions like ReLUs sigmoid functions and Tanhs allow MLPs to determine which nodes to use \nMLPs assist the model to understand the correlation and learn the dependencies between the independent and the target variables from a particular training data set\n  7 Self Organising Maps\n  Professor Teuvo Kohonin invented SOMs or Kohenins map which enable data visualization to reduce the dimensions of data by creating a spatially organised representation\n  It also helps us to understand the correlation between sets of data Data visualization attempts to solve the problem that the human mind cannot easily visualize ie highdimensional data \n  SOMs are created to help users access and understand this highdimensional information\nSOMs dont have activation functions in neurons so they initialize weights for each node and choose a vector at random from the training data\nSOMs examine every node to find which weights are the most likely input vector and the most suitable node is called the Best Matching UnitBMU\nWorking of SOMs\nSOMs discover the crowd around BMUs neighborhood which tends to get lower over time The closer a node is to a BMU the more its weight changes and the winning weight is awarded to the sample vector\n  8 Deep Beliefs Networks\n  DBNs are generative graphical models or a class of deep neural networks that consist of multiple layers of stochastic latent variablesThe latent variables have binary values and are often called hidden units which are connected between the layers        but not within a single layer\n  DBNs are an arrangement of Boltzmann Machines with connections between the layers and in which each RBM layer communicates with both the previous and subsequent layers \nDBNs are used for imagerecognition drug discovery videorecognition and motioncapture data \nGreedyto choose the most optimal option learning algorithms train DBNs The greedy learning algorithm uses an intensive approach of layerbylayer learning of the topdown generative weights\nDBNs run the steps of Gibbs sampling for analysing on the top two hidden layers\nWorking of DBNs\nDBNs draw samples from the visible units using a single pass of ancestral sampling throughout the model \nDBNs learn that the values of the latent variables in every layer can be concluded by a single bottomup pass\n  9 Restricted Boltzmann Machines\n  Developed by Geoffrey Hinton RBMs are stochastic neural networks that possess the capability to learn from a probability distribution over the data ingested\n  RBMs is the founder of many applications in the fields of dimensionality reduction classification and can be trained over supervised or unsupervised data\nThis neural network has applications in regression collaborative filtering feature learning and even many body quantum mechanics \nRBMs are considered to be the building blocks of DBNs\nRBMs consist of two units Visible units and Hidden units Each visible unit is symmetrically connected to all hidden units RBMs consist of a bias unit that is connected to all the visible units and the hidden units but lack output nodes\nWorking of RBFs\nRBMs accept the input and encode it via numbers in the forward passRBMs combine each input with the individuals own weight and one overall bias unit\n  10 Autoencoders\n  Autoencoder is an unsupervised artificial neural network that learns and understands how to efficiently compress and encode data \n  Then learns how to reconstruct the data back from the encoded compression to a representation that is as close to the original input provided at first\nAutoencoders Configuration Source\nAutoencoders are supposed to first encode the image then reduce the size of the input into a smaller entity Finally the autoencoder decodes the image in order to generate the reconstructed image\n    Conclusion\n  All the Deep learning algorithms show us that why are they preferred over other techniques All the algorithms compel us to use deep learning as they have become the norm of the world lately and also serve to our comfort with time effort and ease of use \n  Deep learning has made the working of computers to actually become smart and make them work according to our needs \n  With the ever growing data it can be concluded that these algorithms would only become more efficient with time and would truly be able to replicate the juggernauts of a human brain",'
vietnamese_text = translate_text(english_text)
print(vietnamese_text)
